{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "The models are scored using the Root Mean Squared Logarithmic Error (RMSLE). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utilities import rmsle\n",
    "from utilities import gen_sub\n",
    "\n",
    "# the input files are train2 and test2 because they have been preprocessed\n",
    "trainFile = 'data/train2.csv'\n",
    "testFile = 'data/test2.csv'\n",
    "\n",
    "train = pd.read_csv(trainFile)\n",
    "test = pd.read_csv(testFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Model\n",
    "\n",
    "We begin with the Naive Model: we assume all houses are the mean house price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Model Score:  0.3644525219748948\n"
     ]
    }
   ],
   "source": [
    "naiveTrain_y = pd.read_csv(trainFile)\n",
    "naiveTrain_y['NaivePrediction'] = naiveTrain_y['SalePrice'].mean()\n",
    "\n",
    "naiveScore_train = rmsle(naiveTrain_y['NaivePrediction'], naiveTrain_y['SalePrice'])\n",
    "\n",
    "print(\"Naive Model Score: \", naiveScore_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Naive Model score is about .364, which leaves a lot of room for improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A More Complex Model: Linear Regression\n",
    "\n",
    "We will now use a linear regression model and measure the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first generate a train_test split for evaluating the performance of the model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_linear = train['SalePrice']\n",
    "\n",
    "X_linear = train.drop(columns=['SalePrice'])\n",
    "\n",
    "# we are using a train-test split of 70/30 to ensure enough data is used for training.\n",
    "# the actual test set is the same size as the training set\n",
    "X_linear_train, X_linear_test, y_linear_train, y_linear_test = train_test_split(X_linear, y_linear, \n",
    "                                                                                test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now we train the model on the train split of the training data\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lm = LinearRegression()\n",
    "\n",
    "lm.fit(X_linear_train, y_linear_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Model RMSLE for Validation Set:  0.13807988910042823\n"
     ]
    }
   ],
   "source": [
    "# now we generate predictions on the validation split of the training data and score the predictions\n",
    "#lm_train_pred = lm.predict(X_train)\n",
    "#lm_train_rmsle = rmsle(lm_train_pred, y_train)\n",
    "#print('Linear Model Loss for Training Set: ', lm_train_rmsle)\n",
    "\n",
    "lm_validate_pred = lm.predict(X_linear_test)\n",
    "lm_validate_rmsle = rmsle(lm_validate_pred, y_linear_test)\n",
    "print('Linear Model RMSLE for Validation Set: ', lm_validate_rmsle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear model has a loss of about .138, which is almost 3 times as good at the naive model, which had a loss of about .364.\n",
    "\n",
    "This .138 is very good for a simple linear model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with Hand-Picked Features\n",
    "\n",
    "We will now try a linear regression using only a small subset of features, the features that I beileve to be the most important.\n",
    " - TotalSF\n",
    " - TotalBaths\n",
    " - LotArea\n",
    " - Neighborhood\n",
    " - OverallCond\n",
    " - OverallQual\n",
    " - SaleCondition\n",
    "   -  To catch forclosures, short sales, and sales between family members which would result in significantly lower prices than normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear model with hand picked features\n",
    "most_important_features = ['TotalSF', 'TotalBaths', 'LotArea', \n",
    "                           'OverallCond', 'OverallQual']\n",
    "\n",
    "most_important_features.extend([i for i in list(train) if 'Neighborhood_' in i])\n",
    "most_important_features.extend([i for i in list(train) if 'SaleCondition_' in i])\n",
    "\n",
    "y_linear_hp = train['SalePrice']\n",
    "\n",
    "X_linear_hp = train[most_important_features]\n",
    "\n",
    "X_linear_hp_train, X_linear_hp_test, y_linear_hp_train, y_linear_hp_test = train_test_split(X_linear_hp, y_linear_hp,\n",
    "                                                                                           test_size=.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_hp = LinearRegression()\n",
    "\n",
    "lm_hp.fit(X_linear_hp_train, y_linear_hp_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Model RMSLE for Hand-Picked attributes:  0.15685113282342042\n"
     ]
    }
   ],
   "source": [
    "lm_hp_validate_pred = lm_hp.predict(X_linear_hp_test)\n",
    "lm_hp_validate_rmsle = rmsle(lm_hp_validate_pred, y_linear_hp_test)\n",
    "print('Linear Model RMSLE for Hand-Picked attributes: ', lm_hp_validate_rmsle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear Model with Hand-Picked attributes did not perform better than the Linear Model with all attributes, meaning I may not have selected the best attributes for the linear model.\n",
    "\n",
    "I predict a Lasso Model and a Boosted model will outperform both linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "\n",
    "The Lasso Regression model should select the most important metrics and ignore the least important metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "y_lasso = train['SalePrice']\n",
    "\n",
    "X_lasso = train.drop(columns=['SalePrice'])\n",
    "\n",
    "X_lasso_train, X_lasso_test, y_lasso_train, y_lasso_test = train_test_split(X_lasso, y_lasso,\n",
    "                                                                           test_size=.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the lasso model\n",
    "lasso = LassoCV(alphas=[50, 30, 17.5, 15, 10, 1, 0.1, 0.01, 0.001]).fit(X_lasso_train, y_lasso_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Model RMSLE:  0.11467111358094641\n"
     ]
    }
   ],
   "source": [
    "# get predictions and check accuracy\n",
    "lasso_validate_pred = lasso.predict(X_lasso_test)\n",
    "lasso_validate_rmsle = rmsle(lasso_validate_pred, y_lasso_test)\n",
    "print('Lasso Model RMSLE: ', lasso_validate_rmsle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was able to get the Lasso RMSLE down to .1147 using Hyper-parameter Tuning on the alpha values for the lasso model. This is a very good loss number. The lowest values on Kaggle are in the .109 range, so my result is not too far off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets submit the results of the lasso model to kaggle.\n",
    "lasso_test_pred = lasso.predict(test)\n",
    "lasso_sub = pd.DataFrame(test['Id'])\n",
    "lasso_sub['SalePrice'] = lasso_test_pred\n",
    "\n",
    "lasso_sub = gen_sub(lasso_sub, 'Id', 'SalePrice', filename='lasso_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "y_xgb = train['SalePrice']\n",
    "\n",
    "X_xgb = train.drop(columns=['SalePrice'])\n",
    "\n",
    "X_xgb_train, X_xgb_test, y_xgb_train, y_xgb_test = train_test_split(X_xgb, y_xgb,\n",
    "                                                                   test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model = xgb.XGBRegressor()\n",
    "\n",
    "xgb_model.fit(X_xgb_train, y_xgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Model RMSLE:  0.11692079007432554\n"
     ]
    }
   ],
   "source": [
    "# get predictions and find error\n",
    "xgb_validation_pred = xgb_model.predict(X_xgb_test)\n",
    "xgb_validation_rmsle = rmsle(xgb_validation_pred, y_xgb_test)\n",
    "print('XGBoost Model RMSLE: ', xgb_validation_rmsle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The boosted model performed the second best of all the models tested, with a RMSLE of .1169. Only the Lasso model outperformed the boosted model, and even then, only slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most important features XGBoosted Model:\n",
      "LotArea        : 0.0840\n",
      "OverallQual    : 0.0611\n",
      "OverallCond    : 0.0565\n",
      "TotalSF        : 0.0534\n",
      "Id             : 0.0534\n",
      "GrLivArea      : 0.0473\n",
      "GarageArea     : 0.0473\n",
      "TotalBsmtSF    : 0.0412\n",
      "YearBuilt      : 0.0397\n",
      "GarageYrBlt    : 0.0305\n"
     ]
    }
   ],
   "source": [
    "# get 10 most important features in the dataset.\n",
    "importance = list(xgb_model.feature_importances_)\n",
    "labels = list(train.drop(columns='SalePrice'))\n",
    "\n",
    "mapping = {}\n",
    "for i, j in enumerate(labels):\n",
    "    mapping[j] = importance[i]\n",
    "    \n",
    "sorted_features = sorted([(j,i) for i,j in mapping.items()], reverse=True)\n",
    "print('The 10 most important features XGBoosted Model:')\n",
    "for i in range(10):\n",
    "    print(f'{sorted_features[i][1]:15}: {sorted_features[i][0]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Kaggle submission for the xgboosted model\n",
    "xgb_test_pred = xgb_model.predict(test)\n",
    "xgb_sub = pd.DataFrame(test['Id'])\n",
    "xgb_sub['SalePrice'] = xgb_test_pred\n",
    "\n",
    "xgb_sub = gen_sub(xgb_sub, 'Id', 'SalePrice', filename='xgb_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
